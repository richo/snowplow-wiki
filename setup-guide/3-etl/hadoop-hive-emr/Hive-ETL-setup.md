[**HOME**](Home) > [**SNOWPLOW SETUP GUIDE**](SnowPlow setup guide) > [**ETL**](choosing-an-etl-module) > [**Hive ETL Setup**](hive-etl-setup)

1. [When the Hive ETL module is an appropriate  module to use](#when)
2. [Overview of how it works](#overview)
3. [Setup instructions](#setup)


<a name="when" />
## When is the Hive ETL module an appropriate module to use

The Hive ETL module uses Apache Hive, on Amazon Elastic Mapreduce (EMR), to read the data stored by the [Cloudfront collector](setting-up-the-cloudfront-collector) and transform it into a format suitable either for:

1. Querying directly in Hadoop / Hive using EMR
2. Loading from S3 into a database e.g. [Infobright](setting-up-infobright) for querying and analysis there

This module is therefore suitable if:

1. You using the [Cloudfront collector](setting-up-the-cloudfront-collector)
2. You are using [S3 / Hive](s3-hive-storage-setup) **or** [Infobright](infobright-storage setup) for your storage

<a name="overview" />
## Overview of how it works

The Hive ETL module transforms Cloudfront data in the following steps:

1. It reads the data into Hive from the Cloudfront logs using a [custom deserializer](https://github.com/snowplow/snowplow/tree/master/3-etl/hive-etl/snowplow-log-deserializers). This unpicks the data values stored in the query string
2. It writes the data back to S3 using Hive. If you intend to use Hadoop / Hive for analytics, the data is written back to S3 in a format optimized for processing in Hive / Hadoop. If you intend to load the data into a a database for processing (e.g. Infobright), it writes the data in a tab delimited format, suitable for loading directly into the database. 

The Hive ETL module needs to be run regularly to updating the database with new lines generated by the collector. We recommend running it at least daily, although some SnowPlow users run it more regularly (e.g. hourly).

To make it simpler to setup and run regularly, we've developed the [EMR ETL Runner](https://github.com/snowplow/snowplow/tree/master/3-etl/emr-etl-runner). This is a Ruby application to simply setting up, configuring and scheduling of the ETL on your SnowPlow data in your AWS account.

## Setup instructions

Because the ETL process is wrapped in the [EMR ETL Runner](https://github.com/snowplow/snowplow/tree/master/3-etl/emr-etl-runner), all you need to do is set up the EMR ETL Runner. Instructions on how to do so can be found [here](deploying-the-emretlrunner).